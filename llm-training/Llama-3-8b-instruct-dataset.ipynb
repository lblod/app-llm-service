{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2528 records from data/llm_tasks\\classification_agendapunt_openai_dataset_meta.jsonl\n",
      "Read 17 records from data/llm_tasks\\classification_bpmn_openai_dataset_meta.jsonl\n",
      "Read 1429 records from data/llm_tasks\\translate_agendapunten_openai_dataset_meta.jsonl\n",
      "Read 3099 records from data/llm_tasks\\translate_agendapunt_openai_dataset_meta.jsonl\n",
      "Read 31 records from data/llm_tasks\\translate_bpmn_openai_dataset_meta.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "# function to read the jsonl file containing the training samples with meta data\n",
    "def read_jsonl_file(filepath):\n",
    "    data = []\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping invalid JSON in file {filepath}: {line}\")\n",
    "\n",
    "    print(f\"Read {len(data)} records from {filepath}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def read_jsonl_folder(folder):\n",
    "    data = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.jsonl'):\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            records = read_jsonl_file(filepath)\n",
    "            for record in records:\n",
    "                id = record.get('id')\n",
    "                if id is not None:\n",
    "                    \n",
    "                    data[id] = {\n",
    "                        \"system_message\": record.get('system_message'),\n",
    "                        \"prompt\": record.get('prompt'),\n",
    "                        \"response\": record.get('response'),\n",
    "                        \"category\": record.get('meta', {}).get('task_info', {}).get('category'),\n",
    "                        \"subcategory\": record.get('meta', {}).get('task_info', {}).get('sub_category'),\n",
    "                        \"language\": record.get('meta', {}).get('task_info', {}).get('language'),\n",
    "                    }\n",
    "    return list(data.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting for Training and Inference\n",
    "\n",
    "def format_message(header, message):\n",
    "    \"\"\"\n",
    "    Formats a message in the Llama format.\n",
    "\n",
    "    Parameters:\n",
    "    header (str): The header for the message (e.g., 'system', 'user', 'assistant').\n",
    "    message (str): The message to format.\n",
    "\n",
    "    Returns:\n",
    "    str: The formatted message.\n",
    "    \"\"\"\n",
    "    return f\"<|start_header_id|>{header}<|end_header_id|>{message}<|eot_id|>\"\n",
    "\n",
    "def get_prompt(system_message, prompt):\n",
    "    \"\"\"\n",
    "    Formats a system message and a user prompt in the Llama format.\n",
    "\n",
    "    Parameters:\n",
    "    system_message (str): The system message to format.\n",
    "    prompt (str): The user prompt to format.\n",
    "\n",
    "    Returns:\n",
    "    str: The formatted system message and user prompt.\n",
    "    \"\"\"\n",
    "    return format_message('system', system_message) + format_message('user', prompt)\n",
    "\n",
    "def get_sample(system_message, prompt, output):\n",
    "    \"\"\"\n",
    "    Formats a system message, a user prompt, and an assistant output in the Llama format.\n",
    "\n",
    "    Parameters:\n",
    "    system_message (str): The system message to format.\n",
    "    prompt (str): The user prompt to format.\n",
    "    output (str): The assistant output to format.\n",
    "\n",
    "    Returns:\n",
    "    str: The formatted system message, user prompt, and assistant output.\n",
    "    \"\"\"\n",
    "    return get_prompt(system_message, prompt) + format_message('assistant', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_json_string(json_string):\n",
    "    pattern = r'^```json\\s*(.*?)\\s*```$'\n",
    "    cleaned_string = re.sub(pattern, r'\\1', json_string, flags=re.DOTALL)\n",
    "    return cleaned_string.strip()\n",
    "\n",
    "def clean_tasks(tasks):\n",
    "    cleaned_tasks = []\n",
    "\n",
    "    for task in tasks:\n",
    "        if 'response' not in task or task['response'] is None:\n",
    "            continue\n",
    "        response = clean_json_string(task['response'])\n",
    "        try:\n",
    "            json_response = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipping invalid JSON in response: {response}\")\n",
    "            continue\n",
    "        cleaned_tasks.append(task)\n",
    "\n",
    "    return cleaned_tasks\n",
    "\n",
    "def balance_tasks(tasks, max_per_freq=150):\n",
    "    frequency = {}\n",
    "    balanced_tasks = []\n",
    "\n",
    "    for task in tasks:\n",
    "        response = task['response']\n",
    "        json_response = json.loads(response)\n",
    "        num_translations = len(json_response['translations'])\n",
    "        if num_translations not in frequency:\n",
    "            frequency[num_translations] = 0\n",
    "        if frequency[num_translations] < max_per_freq:\n",
    "            frequency[num_translations] += 1\n",
    "            balanced_tasks.append(task)\n",
    "\n",
    "    sorted_frequency = dict(sorted(frequency.items(), key=lambda item: item[0]))\n",
    "    print(sorted_frequency)\n",
    "\n",
    "    return balanced_tasks\n",
    "\n",
    "def store_tasks(tasks, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for task in tasks:\n",
    "            json.dump(task, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "def clean_balance_and_store_tasks(input_file, output_file, max_per_freq=150):\n",
    "    tasks = read_jsonl_file(input_file)\n",
    "\n",
    "    # Shuffle the tasks\n",
    "    random.shuffle(tasks)\n",
    "\n",
    "    cleaned_tasks = clean_tasks(tasks)\n",
    "    balanced_tasks = balance_tasks(cleaned_tasks, max_per_freq)\n",
    "    store_tasks(balanced_tasks, output_file)\n",
    "\n",
    "# Usage\n",
    "#clean_balance_and_store_tasks(\"data/llm_tasks/translate_agendapunten_openai_dataset.jsonl\", \"data/llm_tasks/translate_agendapunten_openai_dataset_cleaned.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "class InstructDataset(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for creating Instruct Datasets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "        :param dataset: The pandas DataFrame\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def load_dataset(self, dataset: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Load the dataset from the given DataFrame\n",
    "        :param dataset: The pandas DataFrame\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def rename_columns(self, columns: dict[str, str]) -> None:\n",
    "        \"\"\"\n",
    "        Rename the columns of the dataset\n",
    "        :param columns: A dictionary of the form {old_name: new_name}\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.dataset = self.dataset.rename(columns=columns)\n",
    "\n",
    "    def drop_columns(self, columns: list[str]) -> None:\n",
    "        \"\"\"\n",
    "        Drop the columns from the dataset\n",
    "        :param columns: A list of column names to drop\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        drop_columns = [col for col in columns if col in self.dataset.columns]\n",
    "        self.dataset = self.dataset.drop(columns=drop_columns)\n",
    "\n",
    "    def drop_bad_rows(self, columns: list[str]) -> None:\n",
    "        \"\"\"\n",
    "        Drop the rows which have bad values in the columns\n",
    "        :param columns: A list of columns to check for bad values\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.dataset = self.dataset.dropna(subset=columns)\n",
    "        self.dataset = self.dataset.drop_duplicates(subset=columns)\n",
    "\n",
    "    def create_instruction(self, instruction: str) -> None:\n",
    "        \"\"\"\n",
    "        Create an instruction column in the dataset\n",
    "        :param instruction: The instruction to add to the dataset\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.dataset[\"instruction\"] = instruction\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_prompt(self) -> None:\n",
    "        \"\"\"\n",
    "        Create the prompt column in the dataset\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_dataset(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the dataset\n",
    "        :return: The dataset\n",
    "        \"\"\"\n",
    "        return self.dataset\n",
    "\n",
    "class Llama3InstructDataset(InstructDataset):\n",
    "\n",
    "    def create_prompt(self):\n",
    "        \"\"\"\n",
    "        Create the prompt column in the dataset which will be used for\n",
    "        \"\"\"\n",
    "        prompts = []\n",
    "        for index, row in self.dataset.iterrows():\n",
    "            prompt = f\"\"\"<|start_header_id|>system<|end_header_id|> {row['instruction']}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "              This is the question: {row['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|> {row['output']}<|eot_id|>\"\"\"\n",
    "            prompts.append(prompt)\n",
    "        self.dataset[\"prompt\"] = prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ec3231427d4887867849a445f139f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2528 records from data/llm_tasks\\classification_agendapunt_openai_dataset_meta.jsonl\n",
      "Read 17 records from data/llm_tasks\\classification_bpmn_openai_dataset_meta.jsonl\n",
      "Read 1429 records from data/llm_tasks\\translate_agendapunten_openai_dataset_meta.jsonl\n",
      "Read 3099 records from data/llm_tasks\\translate_agendapunt_openai_dataset_meta.jsonl\n",
      "Read 31 records from data/llm_tasks\\translate_bpmn_openai_dataset_meta.jsonl\n",
      "                                      system_message  \\\n",
      "0  Your task is to generate responses in JSON for...   \n",
      "1  Your task is to generate responses in JSON for...   \n",
      "2  Your task is to generate responses in JSON for...   \n",
      "3  Your task is to generate responses in JSON for...   \n",
      "4  Your task is to generate responses in JSON for...   \n",
      "\n",
      "                                              prompt  \\\n",
      "0  ####\\nContext: {\"uri\": \"https://hoogstraten.me...   \n",
      "1  ####\\nContext: [{\"uri\": \"http://data.lblod.inf...   \n",
      "2  ####\\nContext: {\"uri\": \"http://data.lblod.info...   \n",
      "3  ####\\nContext: {\"uri\": \"https://lblod.bredene....   \n",
      "4  ####\\nContext: {\"uri\": \"https://data.leuven.be...   \n",
      "\n",
      "                                            response     category  \\\n",
      "0  ```json\\n{\\n    \"uri\": \"Do not translate\",\\n  ...  translation   \n",
      "1  ```json\\n{\\n  \"translations\": [\\n    {\\n      ...  translation   \n",
      "2  ```json\\n{\\n    \"uri\": \"http://data.lblod.info...  translation   \n",
      "3  ```json\\n{\\n    \"uri\": \"https://lblod.bredene....  translation   \n",
      "4  ```json\\n{\\n    \"uri\": \"https://data.leuven.be...  translation   \n",
      "\n",
      "              subcategory language  \n",
      "0    translate_agendapunt   German  \n",
      "1  translate_agendapunten  English  \n",
      "2    translate_agendapunt   French  \n",
      "3    translate_agendapunt   German  \n",
      "4    translate_agendapunt   French  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Define the folder containing the data\n",
    "folder = \"data/llm_tasks\"\n",
    "\n",
    "# Read the data from the folder\n",
    "data = read_jsonl_folder(folder)\n",
    "\n",
    "# Convert the data to a pandas DataFrame and shuffle it\n",
    "finetuning_df = pd.DataFrame(data)\n",
    "finetuning_df = finetuning_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(finetuning_df.head(5))\n",
    "\n",
    "# Create a Llama3InstructDataset from the DataFrame\n",
    "dataset_abb = Llama3InstructDataset(finetuning_df)\n",
    "\n",
    "# Define the columns to remove and the new names for the remaining columns\n",
    "REMOVE_COLUMNS = []\n",
    "RENAME_COLUMNS = {\"system_message\": \"instruction\",\"prompt\":\"input\", \"response\": \"output\"}\n",
    "\n",
    "def process_dataset(dataset) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process the instruct dataset to be in the format required by the model.\n",
    "    :param dataset: The dataset to process.\n",
    "    :return: The processed dataset.\n",
    "    \"\"\"\n",
    "    # Remove unnecessary columns\n",
    "    dataset.drop_columns(REMOVE_COLUMNS)\n",
    "    logger.info(\"Columns removed!\")\n",
    "\n",
    "    # Rename the remaining columns\n",
    "    dataset.rename_columns(RENAME_COLUMNS)\n",
    "    logger.info(\"Columns renamed!\")\n",
    "\n",
    "    # Drop rows with missing values in the 'input' and 'output' columns\n",
    "    dataset.drop_bad_rows([\"input\", \"output\"])\n",
    "    logger.info(\"Bad rows dropped!\")\n",
    "\n",
    "    # Create the prompt column\n",
    "    dataset.create_prompt()\n",
    "    logger.info(\"Prompt column created!\")\n",
    "\n",
    "    return dataset.get_dataset()\n",
    "\n",
    "def create_dataset_hf(dataset: pd.DataFrame) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Create a Hugging Face dataset from the pandas dataframe.\n",
    "    :param dataset: The pandas dataframe.\n",
    "    :return: The Hugging Face dataset.\n",
    "    \"\"\"\n",
    "    # Shuffle the dataset\n",
    "    dataset.sample(frac=1)\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Convert the pandas DataFrame to a Hugging Face Dataset\n",
    "    dataset_hf = Dataset.from_pandas(dataset)\n",
    "\n",
    "    # Split the dataset into train, test, and validation sets\n",
    "    train_test_split = dataset_hf.train_test_split(test_size=0.1)\n",
    "    train_val_split = train_test_split['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": train_val_split['train'],\n",
    "        \"test\": train_test_split['test'],\n",
    "        \"validation\": train_val_split['test']\n",
    "    })\n",
    "\n",
    "# Process the dataset and create a Hugging Face dataset from it\n",
    "dataset = process_dataset(dataset_abb)\n",
    "dataset_hf = create_dataset_hf(dataset)\n",
    "\n",
    "# Uncomment the following line to push the dataset to the Hugging Face Hub\n",
    "# dataset_hf.push_to_hub(f\"llama3_{dataset_name}_instruct_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
