{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Breakdown of the ABB-LLM Model:\n",
    "\n",
    "## Motivation:\n",
    "* As a Tool for Translation, Summarization, QA Tasks:These tasks require the generation of new text.\n",
    "* As a Baseline for Classification, Named Entity Recognition (NER), and Other Tasks: These tasks require understanding of the text.\n",
    "\n",
    "## What to Expect?\n",
    "\n",
    "Small LLM Models:\n",
    "* While not as good as larger models, they can still be useful for many tasks.\n",
    "* Current 7B models are not good enough for QA tasks. Small models tend to hallucinate more and are less accurate, thus general QA tasks are not advised.\n",
    "* It is recommended to stick to summarization, translation, and classification tasks until better models are available.\n",
    "* For tasks that take in a 'context' (document, search result, etc.) and only use the information in the context to generate the output (e.g., summarization, translation, classification, NER), small models can be effective.\n",
    "* he svercoutere/llama-3-8b-instruct-abb model is fine-tuned specifically for these 'simpler' tasks and should perform better than the base 8B model.\n",
    "* It is trained to return JSON output, which is easier to work with than the default output of the 8B model, which tends to add a lot of noise or unnecessary information (being chatty).\n",
    "\n",
    "\n",
    "### Use Cases (all task where the context/facts are provided):\n",
    "* Summarization: Summaries of any text (e.g., agenda items, BPMN files).\n",
    "* Translation: Simple translations of text (e.g., agenda items, BPMN files).\n",
    "* Classification: Classify text into any hierarchy (e.g., agenda items, BPMN files).\n",
    "* Named Entity Recognition (NER): Extract entities from text (e.g., agenda items, BPMN files).\n",
    "* Keyword Extraction: Extract keywords from text (e.g., agenda items, BPMN files).\n",
    "\n",
    "\n",
    "# Long Term:\n",
    "* When enough data is available, a custom model should be trained for specific tasks.\n",
    "* While a (small) LLM can be used, a custom model for classification and NER tasks should function more efficiently and perform better than a model trained on general tasks.\n",
    "* This can be easily achieved by fine-tuning models such as BERT, RoBERTa, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "Parameter Efficient Fine-Tuning (PEFT) is a resource-efficient alternative to full fine-tuning for instruction-based large language models (LLMs). Unlike full fine-tuning, which involves updating all model parameters and requires significant computational resources, PEFT updates only a subset of parameters, keeping the rest frozen. This reduces the number of trainable parameters, thus lowering memory requirements and preserving the original LLM weights to prevent catastrophic forgetting. PEFT is particularly useful for mitigating storage constraints when fine-tuning across multiple tasks. Techniques such as Low-Rank Adaptation (LoRA) and Quantized Low-Rank Adaptation (QLoRA) exemplify effective methods for PEFT.\n",
    "\n",
    "# LoRA and QLoRA\n",
    "\n",
    "**LoRA (Low-Rank Adaptation):** LoRA fine-tunes two smaller matrices that approximate the pre-trained LLM's weight matrix, forming a LoRA adapter. After fine-tuning, the original LLM remains unchanged, while the LoRA adapter is significantly smaller (measured in MB rather than GB). During inference, the LoRA adapter is fused with the original LLM, allowing multiple LoRA adapters to repurpose the LLM for different tasks, reducing overall memory requirements.\n",
    "\n",
    "**QLoRA (Quantized Low-Rank Adaptation):** QLoRA improves upon LoRA by quantizing the LoRA adapter weights to lower precision, typically 4-bit instead of 8-bit. This further reduces the memory footprint and storage overhead. Despite the reduced bit precision, QLoRA maintains performance levels comparable to LoRA, optimizing memory usage without compromising effectiveness.\n",
    "\n",
    "# Full Fine-Tuning vs. PEFT-LoRA vs. PEFT-QLoRA\n",
    "\n",
    "- **Full Fine-Tuning:** Updates all model parameters, requires substantial computational resources, and has high memory and storage demands.\n",
    "- **PEFT-LoRA:** Updates only small matrices, preserving most original weights, reducing memory needs, and allowing easy adaptation for multiple tasks.\n",
    "- **PEFT-QLoRA:** Further reduces memory and storage by quantizing weights, maintaining effective performance while optimizing resource usage.\n",
    "\n",
    "![Different approaches for training an LLM](./llm_finetuning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!mamba install --force-reinstall aiohttp -y\n",
    "!pip install -U \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install \"unsloth @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n",
    "!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration of unsloth\n",
    "dataset_repo = \"svercoutere/llama3_abb_instruct_dataset\"  # The dataset repository\n",
    "\n",
    "config = {\n",
    "    \"model_config\": {\n",
    "        \"base_model\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",  # The base model\n",
    "        \"max_seq_length\": 8096,  # The maximum sequence length\n",
    "        \"dtype\": None,  # The data type\n",
    "        \"load_in_4bit\": True,  # Load the model in 4-bit\n",
    "    },\n",
    "    \"lora_config\": {\n",
    "        \"r\": 8,  # The number of LoRA layers 8, 16, 32, 64\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # The target modules\n",
    "        \"lora_alpha\": 16,  # The alpha value for LoRA\n",
    "        \"lora_dropout\": 0,  # The dropout value for LoRA\n",
    "        \"bias\": \"none\",  # The bias for LoRA\n",
    "        \"use_gradient_checkpointing\": True,  # Use gradient checkpointing\n",
    "        \"random_state\": 3407,  # The random state\n",
    "        \"use_rslora\": False,  # Use RSLora\n",
    "        \"loftq_config\": None  # The LoFTQ configuration\n",
    "    },\n",
    "    \"training_config\":  {\n",
    "        \"dataset_input_field\": \"prompt\",  # The input field\n",
    "        \n",
    "        \"per_device_train_batch_size\": 2,  # The batch size\n",
    "        \"gradient_accumulation_steps\": 4,  # The gradient accumulation steps\n",
    "        \"warmup_steps\": 5,  # The warmup steps\n",
    "        \"max_steps\": 0,  # The maximum steps (0 if the epochs are defined)\n",
    "        \"num_train_epochs\": 1,  # The number of training epochs(0 if the maximum steps are defined)\n",
    "        \"learning_rate\": 2e-4,  # The learning rate\n",
    "        \"logging_steps\": 10,  # The logging steps\n",
    "\n",
    "        \"eval_strategy\": \"steps\",  # The evaluation strategy\n",
    "        \"per_device_eval_batch_size\": 2,  # The batch size for evaluation\n",
    "        \"eval_steps\": 10,  # The evaluation steps\n",
    "\n",
    "        \"save_strategy\": \"steps\",  # The save strategy\n",
    "        \"save_steps\": 10,  # The save steps\n",
    "        \"save_total_limit\": 5,  # The total limit for saving\n",
    "        \"resume_from_checkpoint\": \"outputs/checkpoint-208\",  # The checkpoint to resume from\n",
    "        \n",
    "        \"optim\": \"adamw_8bit\",  # The optimizer\n",
    "        \"weight_decay\": 0.01,  # The weight decay\n",
    "        \"lr_scheduler_type\": \"linear\",  # The learning rate scheduler\n",
    "        \"seed\": 3407,  # The seed\n",
    "        \"output_dir\": \"outputs\",  # The output directory\n",
    "        \"report_to\": \"none\",  # The report destination\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=config[\"model_config\"][\"base_model\"],\n",
    "        max_seq_length=config[\"model_config\"][\"max_seq_length\"],\n",
    "        dtype=config[\"model_config\"][\"dtype\"],\n",
    "        load_in_4bit=config[\"model_config\"][\"load_in_4bit\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=config[\"lora_config\"][\"r\"],\n",
    "        target_modules=config[\"lora_config\"][\"target_modules\"],\n",
    "        lora_alpha=config[\"lora_config\"][\"lora_alpha\"],\n",
    "        lora_dropout=config[\"lora_config\"][\"lora_dropout\"],\n",
    "        bias=config[\"lora_config\"][\"bias\"],\n",
    "        use_gradient_checkpointing=config[\"lora_config\"][\"use_gradient_checkpointing\"],\n",
    "        random_state=config[\"lora_config\"][\"random_state\"],\n",
    "        use_rslora=config[\"lora_config\"][\"use_rslora\"],\n",
    "        loftq_config=config[\"lora_config\"][\"loftq_config\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Instruction Data for Llama 3 8B Instruct\n",
    "\n",
    "To fully utilize the Llama 3 8B Instruct model, we need to adhere to its prompt instruction template:\n",
    "\n",
    "```\n",
    "\n",
    "<|start_header_id|>system<|end_header_id|>{ system_message }<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>{ prompt }<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>{ output }<|eot_id|>\n",
    "\n",
    "```\n",
    "\n",
    "The dataset [svercoutere/llama3_abb_instruct_dataset](https://huggingface.co/datasets/svercoutere/llama3_abb_instruct_dataset) is LLM-agnostic and does not have the data in the format required by the model. We need to prepare the data in the required format. Refer to the `add_prompt_to_dataset` function for detailed implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# adding the prompt to the dataset in the llama-3-8b-instruct format\n",
    "\n",
    "def get_prompt(system_message, prompt):    \n",
    "    llama_format = f\"\"\"<|start_header_id|>system<|end_header_id|>{ system_message }<|eot_id|><|start_header_id|>user<|end_header_id|>{ prompt }<|eot_id|>\"\"\"\n",
    "    return llama_format\n",
    "\n",
    "def get_prompt_training(system_message, prompt, output):\n",
    "    llama_format = f\"\"\"<|start_header_id|>system<|end_header_id|>{ system_message }<|eot_id|><|start_header_id|>user<|end_header_id|>{ prompt }<|eot_id|><|start_header_id|>assistant<|end_header_id|>{ output }<|eot_id|>\"\"\"\n",
    "    return llama_format\n",
    "\n",
    "def add_prompt_to_dataset(dataset):\n",
    "    dataset = dataset.map(lambda x: {\"prompt\": get_prompt_training(x[\"instruction\"], x[\"input\"], x[\"output\"])}, batched=True,)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset_train = load_dataset(dataset_repo, split = \"train\")\n",
    "dataset_val = load_dataset(dataset_repo, split = \"validation\")\n",
    "\n",
    "dataset_train = add_prompt_to_dataset(dataset_train)\n",
    "dataset_val = add_prompt_to_dataset(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    dataset_text_field=config[\"training_dataset\"][\"input_field\"],\n",
    "    max_seq_length=config[\"model_config\"][\"max_seq_length\"],\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=config[\"training_config\"][\"per_device_train_batch_size\"],\n",
    "        gradient_accumulation_steps=config[\"training_config\"][\"gradient_accumulation_steps\"],\n",
    "        warmup_steps=config[\"training_config\"][\"warmup_steps\"],\n",
    "        max_steps=config[\"training_config\"][\"max_steps\"],\n",
    "        num_train_epochs=config[\"training_config\"][\"num_train_epochs\"],\n",
    "        learning_rate=config[\"training_config\"][\"learning_rate\"],\n",
    "        fp16=not torch.cuda.is_bf16_supported(), # Add it here to avoid the error with tensors being on different devices\n",
    "        bf16=torch.cuda.is_bf16_supported(), # Add it here to avoid the error with tensors being on different devices\n",
    "        logging_steps=config[\"training_config\"][\"logging_steps\"],\n",
    "        eval_strategy=config[\"training_config\"][\"eval_strategy\"],\n",
    "        per_device_eval_batch_size=config[\"training_config\"][\"per_device_eval_batch_size\"],\n",
    "        eval_steps=config[\"training_config\"][\"eval_steps\"],\n",
    "        save_strategy=config[\"training_config\"][\"save_strategy\"],\n",
    "        save_steps=config[\"training_config\"][\"save_steps\"],\n",
    "        save_total_limit=config[\"training_config\"][\"save_total_limit\"],\n",
    "        resume_from_checkpoint=config[\"training_config\"][\"resume_from_checkpoint\"],\n",
    "        optim=config[\"training_config\"][\"optim\"],\n",
    "        weight_decay=config[\"training_config\"][\"weight_decay\"],\n",
    "        lr_scheduler_type=config[\"training_config\"][\"lr_scheduler_type\"],\n",
    "        seed=config[\"training_config\"][\"seed\"],\n",
    "        output_dir=config[\"training_config\"][\"output_dir\"],\n",
    "        report_to=config[\"training_config\"][\"report_to\"],\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory statistics before training\n",
    "gpu_statistics = torch.cuda.get_device_properties(0)\n",
    "reserved_memory = round(torch.cuda.max_memory_reserved() / 1024**3, 2)\n",
    "max_memory = round(gpu_statistics.total_memory / 1024**3, 2)\n",
    "print(f\"Reserved Memory: {reserved_memory}GB\")\n",
    "print(f\"Max Memory: {max_memory}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory(model):\n",
    "    import gc\n",
    "\n",
    "    model.cpu()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "#clear_gpu_memory(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer_stats = trainer.train(resume_from_checkpoint = True)\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory statistics after training\n",
    "used_memory = round(torch.cuda.max_memory_allocated() / 1024**3, 2)\n",
    "used_memory_lora = round(used_memory - reserved_memory, 2)\n",
    "used_memory_persentage = round((used_memory / max_memory) * 100, 2)\n",
    "used_memory_lora_persentage = round((used_memory_lora / max_memory) * 100, 2)\n",
    "print(f\"Used Memory: {used_memory}GB ({used_memory_persentage}%)\")\n",
    "print(f\"Used Memory for training(fine-tuning) LoRA: {used_memory_lora}GB ({used_memory_lora_persentage}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally saving the model and pushing it to the Hugging Face Hub (only LoRA adapters)\n",
    "model.save_pretrained(\"llama-3-8b-instruct-abb\")\n",
    "model.push_to_hub(\"llama-3-8b-instruct-abb\", tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model using merged_16bit(float16), merged_4bit(int4) or quantization options(q8_0, q4_k_m, q5_k_m)...\n",
    "\n",
    "model.save_pretrained_merged(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, save_method = \"merged_16bit\",)\n",
    "model.push_to_hub_merged(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, save_method = \"merged_16bit\")\n",
    "\n",
    "model.save_pretrained_merged(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, save_method = \"merged_4bit\",)\n",
    "model.push_to_hub_merged(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, save_method = \"merged_4bit\")\n",
    "\n",
    "model.save_pretrained_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer)\n",
    "model.push_to_hub_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer)\n",
    "\n",
    "model.save_pretrained_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"f16\")\n",
    "model.push_to_hub_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"f16\")\n",
    "\n",
    "model.save_pretrained_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"q4_k_m\")\n",
    "model.push_to_hub_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"llama-3-8b-instruct-abb\",\n",
    "        max_seq_length = 4096,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "\n",
    "# Using FastLanguageModel for fast inference\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    texts = []\n",
    "    for instruction, input in zip(instructions, inputs):\n",
    "        text = get_prompt(instruction, input)\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "\n",
    "dataset_test = load_dataset(\"svercoutere/llama3_abb_instruct_dataset\", split = \"train\")\n",
    "dataset_test = dataset_test.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt):\n",
    "    inputs = tokenizer(\n",
    "    [prompt], return_tensors = \"pt\", padding = True).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True, temperature = 0.0)\n",
    "    print(\"tokens(total):\",len(outputs[0]), \"tokens(prompt):\", len(inputs[0]))\n",
    "    \n",
    "    new_tokens = outputs[0][len(inputs[0]):]\n",
    "    print(\"tokens(new):\",len(new_tokens))\n",
    "    return tokenizer.batch_decode([new_tokens], skip_special_tokens = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(5,20):\n",
    "    print(\"\\n---------------Sample:\",str(index),\"----------------------\")\n",
    "\n",
    "    print(\"Input:\")\n",
    "    print(dataset_test[index][\"input\"].split(\"####\")[1])\n",
    "\n",
    "    response = generate_text(dataset_test[index][\"text\"])[0]\n",
    "    print(\"Prediction:\\n\")\n",
    "    print(response.replace(\"<|start_header_id|>assistant<|end_header_id|>\",\"\").replace(\"<|eot_id|>\",\"\").strip())\n",
    "    print(\"Expected:\\n\")\n",
    "    print(dataset_test[index][\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
